
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>6. Pretraining on Unlabeled Data &#8212; Large Language Models</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/drawio.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script src="https://wavedrom.com/skins/default.js"></script>
    <script src="https://wavedrom.com/wavedrom.min.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
    <script src="https://cdn.jsdelivr.net/gh/bonartm/quizdown-js@latest/public/build/quizdown.js"></script>
    <script>quizdown.init({"quizdown_js": "https://cdn.jsdelivr.net/gh/bonartm/quizdown-js@latest/public/build/quizdown.js"});</script>
    <link rel="shortcut icon" href="../_static/favicon_llm.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="7. Finetuning for Text Classification" href="../finetuning/finetuning-for-text-classification.html" />
    <link rel="prev" title="5. Implementing a GPT model" href="implementing-a-GPT-model.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo_llm.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Large Language Models</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  LLM Intro
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../basic/basic.html">
   1. Large Language Models Basic
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../basic/attention.html">
   2. Attention Mechanisms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../basic/transformer.html">
   3. Transformer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../basic/language-modelling.html">
   4. Transformers for Language Modelling
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Pre-trained Model
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="implementing-a-GPT-model.html">
   5. Implementing a GPT model
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   6. Pretraining on Unlabeled Data
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Finetuning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../finetuning/finetuning-for-text-classification.html">
   7. Finetuning for Text Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../finetuning/qlora-llm-instruct-fine-tuning-flan-t5-large.html">
   8. Fine-Tuning Lora
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Operation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../operation/chatwithPDF.html">
   9. Chat with PDFs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../operation/ranked-predictions-with-bert.html">
   10. Ranked Predictions
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Assignments
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../assignments/README.html">
   11. Self-paced assignments
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../assignments/transformer-architecture.html">
   12. Complete the transformer architecture
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../assignments/llama3-finetune.html">
   13. Fine-tuning Llama3
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/penjc/llmbook/main?urlpath=lab/tree/llmbook/pretrained-model/pretraining-on-unlabeled-data.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/penjc/llmbook/blob/main/llmbook/pretrained-model/pretraining-on-unlabeled-data.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
      <li>
        
<button onclick="initThebeSBT()"
  class="headerbtn headerbtn-launch-thebe"
  data-toggle="tooltip"
data-placement="left"
title="Launch Thebe"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="headerbtn__text-container">Live Code</span>
</button>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/penjc/llmbook"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/penjc/llmbook/issues/new?title=Issue%20on%20page%20%2Fpretrained-model/pretraining-on-unlabeled-data.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/penjc/llmbook/edit/main/llmbook/pretrained-model/pretraining-on-unlabeled-data.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/pretrained-model/pretraining-on-unlabeled-data.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluating-generative-text-models">
   6.1. Evaluating generative text models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-gpt-to-generate-text">
     6.1.1. Using GPT to generate text
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#calculating-the-text-generation-loss-cross-entropy-and-perplexity">
     6.1.2. Calculating the text generation loss: cross entropy, and perplexity
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#calculating-the-training-and-validation-set-losses">
     6.1.3. Calculating the training and validation set losses
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training-an-llm">
   6.2. Training an LLM
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#decoding-strategies-to-control-randomness">
   6.3. Decoding strategies to control randomness
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#temperature-scaling">
     6.3.1. Temperature scaling
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#top-k-sampling">
     6.3.2. Top-k sampling
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#modifying-the-text-generation-function">
     6.3.3. Modifying the text generation function
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#loading-and-saving-model-weights-in-pytorch">
   6.4. Loading and saving model weights in PyTorch
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#loading-pretrained-weights-from-open-ai">
   6.5. Loading pretrained weights from Open AI
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#acknowledgments">
   6.6. Acknowledgments
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Pretraining on Unlabeled Data</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluating-generative-text-models">
   6.1. Evaluating generative text models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-gpt-to-generate-text">
     6.1.1. Using GPT to generate text
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#calculating-the-text-generation-loss-cross-entropy-and-perplexity">
     6.1.2. Calculating the text generation loss: cross entropy, and perplexity
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#calculating-the-training-and-validation-set-losses">
     6.1.3. Calculating the training and validation set losses
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training-an-llm">
   6.2. Training an LLM
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#decoding-strategies-to-control-randomness">
   6.3. Decoding strategies to control randomness
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#temperature-scaling">
     6.3.1. Temperature scaling
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#top-k-sampling">
     6.3.2. Top-k sampling
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#modifying-the-text-generation-function">
     6.3.3. Modifying the text generation function
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#loading-and-saving-model-weights-in-pytorch">
   6.4. Loading and saving model weights in PyTorch
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#loading-pretrained-weights-from-open-ai">
   6.5. Loading pretrained weights from Open AI
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#acknowledgments">
   6.6. Acknowledgments
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="pretraining-on-unlabeled-data">
<h1><span class="section-number">6. </span>Pretraining on Unlabeled Data<a class="headerlink" href="#pretraining-on-unlabeled-data" title="Permalink to this headline">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">importlib.metadata</span><span class="w"> </span><span class="kn">import</span> <span class="n">version</span>

<span class="n">pkgs</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;matplotlib&quot;</span><span class="p">,</span> 
        <span class="s2">&quot;numpy&quot;</span><span class="p">,</span> 
        <span class="s2">&quot;tiktoken&quot;</span><span class="p">,</span> 
        <span class="s2">&quot;torch&quot;</span><span class="p">,</span>
        <span class="s2">&quot;tensorflow&quot;</span> <span class="c1"># For OpenAI&#39;s pretrained weights</span>
       <span class="p">]</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">pkgs</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">p</span><span class="si">}</span><span class="s2"> version: </span><span class="si">{</span><span class="n">version</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>matplotlib version: 3.8.2
numpy version: 1.26.0
tiktoken version: 0.5.1
torch version: 2.2.2
tensorflow version: 2.15.0
</pre></div>
</div>
</div>
</div>
<p>In this chapter, we implement the training loop and code for basic model evaluation to pretrain an LLM
At the end of this chapter, we also load openly available pretrained weights from OpenAI into our model</p>
<img src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/llm/clipboard_20240409_035227.png" width=500px><p>The topics covered in this chapter are shown below</p>
<section id="evaluating-generative-text-models">
<h2><span class="section-number">6.1. </span>Evaluating generative text models<a class="headerlink" href="#evaluating-generative-text-models" title="Permalink to this headline">#</a></h2>
<p>We start this section with a brief recap of initializing a GPT model using the code from the previous chapter
Then, we discuss basic evaluation metrics for LLMs
Lastly, in this section, we apply these evaluation metrics to a training and validation dataset</p>
<section id="using-gpt-to-generate-text">
<h3><span class="section-number">6.1.1. </span>Using GPT to generate text<a class="headerlink" href="#using-gpt-to-generate-text" title="Permalink to this headline">#</a></h3>
<p>We initialize a GPT model using the code from the previous chapter</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Some relevant codes</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">tiktoken</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DataLoader</span>

<span class="k">class</span><span class="w"> </span><span class="nc">GPTDatasetV1</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">txt</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_ids</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_ids</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Tokenize the entire text</span>
        <span class="n">token_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">txt</span><span class="p">)</span>

        <span class="c1"># Use a sliding window to chunk the book into overlapping sequences of max_length</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span> <span class="o">-</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
            <span class="n">input_chunk</span> <span class="o">=</span> <span class="n">token_ids</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">max_length</span><span class="p">]</span>
            <span class="n">target_chunk</span> <span class="o">=</span> <span class="n">token_ids</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">max_length</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">input_chunk</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">target_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">target_chunk</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_ids</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_ids</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_ids</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>


<span class="k">def</span><span class="w"> </span><span class="nf">create_dataloader_v1</span><span class="p">(</span><span class="n">txt</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
                         <span class="n">stride</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="c1"># Initialize the tokenizer</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tiktoken</span><span class="o">.</span><span class="n">get_encoding</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>

    <span class="c1"># Create dataset</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">GPTDatasetV1</span><span class="p">(</span><span class="n">txt</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">stride</span><span class="p">)</span>

    <span class="c1"># Create dataloader</span>
    <span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="n">shuffle</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="n">drop_last</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">dataloader</span>

<span class="k">class</span><span class="w"> </span><span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">qkv_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">d_out</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;d_out must be divisible by n_heads&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">d_out</span> <span class="o">=</span> <span class="n">d_out</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">d_out</span> <span class="o">//</span> <span class="n">num_heads</span>  <span class="c1"># Reduce the projection dim to match desired output dim</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">W_query</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">qkv_bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_key</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">qkv_bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_value</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">qkv_bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_out</span><span class="p">,</span> <span class="n">d_out</span><span class="p">)</span>  <span class="c1"># Linear layer to combine head outputs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;mask&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">block_size</span><span class="p">,</span> <span class="n">block_size</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">b</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">,</span> <span class="n">d_in</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="n">keys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_key</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Shape: (b, num_tokens, d_out)</span>
        <span class="n">queries</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_query</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_value</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># We implicitly split the matrix by adding a `num_heads` dimension</span>
        <span class="c1"># Unroll last dim: (b, num_tokens, d_out) -&gt; (b, num_tokens, num_heads, head_dim)</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="n">keys</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">queries</span> <span class="o">=</span> <span class="n">queries</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>

        <span class="c1"># Transpose: (b, num_tokens, num_heads, head_dim) -&gt; (b, num_heads, num_tokens, head_dim)</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="n">keys</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">queries</span> <span class="o">=</span> <span class="n">queries</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Compute scaled dot-product attention (aka self-attention) with a causal mask</span>
        <span class="n">attn_scores</span> <span class="o">=</span> <span class="n">queries</span> <span class="o">@</span> <span class="n">keys</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># Dot product for each head</span>

        <span class="c1"># Original mask truncated to the number of tokens and converted to boolean</span>
        <span class="n">mask_bool</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask</span><span class="o">.</span><span class="n">bool</span><span class="p">()[:</span><span class="n">num_tokens</span><span class="p">,</span> <span class="p">:</span><span class="n">num_tokens</span><span class="p">]</span>

        <span class="c1"># Use the mask to fill attention scores</span>
        <span class="n">attn_scores</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">mask_bool</span><span class="p">,</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>

        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_scores</span> <span class="o">/</span> <span class="n">keys</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">)</span>

        <span class="c1"># Shape: (b, num_tokens, num_heads, head_dim)</span>
        <span class="n">context_vec</span> <span class="o">=</span> <span class="p">(</span><span class="n">attn_weights</span> <span class="o">@</span> <span class="n">values</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Combine heads, where self.d_out = self.num_heads * self.head_dim</span>
        <span class="n">context_vec</span> <span class="o">=</span> <span class="n">context_vec</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_out</span><span class="p">)</span>
        <span class="n">context_vec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">context_vec</span><span class="p">)</span>  <span class="c1"># optional projection</span>

        <span class="k">return</span> <span class="n">context_vec</span>
      
<span class="k">class</span><span class="w"> </span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-5</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">emb_dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shift</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">emb_dim</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">var</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">unbiased</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">norm_x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">*</span> <span class="n">norm_x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">shift</span>


<span class="k">class</span><span class="w"> </span><span class="nc">GELU</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">pi</span><span class="p">))</span> <span class="o">*</span>
            <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mf">0.044715</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
        <span class="p">))</span>


<span class="k">class</span><span class="w"> </span><span class="nc">FeedForward</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">],</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">]),</span>
            <span class="n">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">],</span> <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">]),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;drop_rate&quot;</span><span class="p">])</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">TransformerBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">att</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span>
            <span class="n">d_in</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">],</span>
            <span class="n">d_out</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">],</span>
            <span class="n">block_size</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;ctx_len&quot;</span><span class="p">],</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;n_heads&quot;</span><span class="p">],</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;drop_rate&quot;</span><span class="p">],</span>
            <span class="n">qkv_bias</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;qkv_bias&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ff</span> <span class="o">=</span> <span class="n">FeedForward</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">drop_resid</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;drop_rate&quot;</span><span class="p">])</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Shortcut connection for attention block</span>
        <span class="n">shortcut</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">att</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>   <span class="c1"># Shape [batch_size, num_tokens, emb_size]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_resid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">shortcut</span>  <span class="c1"># Add the original input back</span>

        <span class="c1"># Shortcut connection for feed-forward block</span>
        <span class="n">shortcut</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_resid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">shortcut</span>  <span class="c1"># Add the original input back</span>

        <span class="k">return</span> <span class="n">x</span>


<span class="k">class</span><span class="w"> </span><span class="nc">GPTModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tok_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;vocab_size&quot;</span><span class="p">],</span> <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;ctx_len&quot;</span><span class="p">],</span> <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">drop_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;drop_rate&quot;</span><span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">trf_blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="o">*</span><span class="p">[</span><span class="n">TransformerBlock</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;n_layers&quot;</span><span class="p">])])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">final_norm</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">],</span> <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;vocab_size&quot;</span><span class="p">],</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_idx</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span> <span class="o">=</span> <span class="n">in_idx</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">tok_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tok_emb</span><span class="p">(</span><span class="n">in_idx</span><span class="p">)</span>
        <span class="n">pos_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_emb</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">in_idx</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tok_embeds</span> <span class="o">+</span> <span class="n">pos_embeds</span>  <span class="c1"># Shape [batch_size, num_tokens, emb_size]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_emb</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">final_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span>


<span class="k">def</span><span class="w"> </span><span class="nf">generate_text_simple</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="p">,</span> <span class="n">context_size</span><span class="p">):</span>
    <span class="c1"># idx is (B, T) array of indices in the current context</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="p">):</span>

        <span class="c1"># Crop current context if it exceeds the supported context size</span>
        <span class="c1"># E.g., if LLM supports only 5 tokens, and the context size is 10</span>
        <span class="c1"># then only the last 5 tokens are used as context</span>
        <span class="n">idx_cond</span> <span class="o">=</span> <span class="n">idx</span><span class="p">[:,</span> <span class="o">-</span><span class="n">context_size</span><span class="p">:]</span>

        <span class="c1"># Get the predictions</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">idx_cond</span><span class="p">)</span>

        <span class="c1"># Focus only on the last time step</span>
        <span class="c1"># (batch, n_token, vocab_size) becomes (batch, vocab_size)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>

        <span class="c1"># Get the idx of the vocab entry with the highest logits value</span>
        <span class="n">idx_next</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># (batch, 1)</span>

        <span class="c1"># Append sampled index to the running sequence</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx_next</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch, n_tokens+1)</span>

    <span class="k">return</span> <span class="n">idx</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>

    <span class="n">GPT_CONFIG_124M</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">50257</span><span class="p">,</span>  <span class="c1"># Vocabulary size</span>
        <span class="s2">&quot;ctx_len&quot;</span><span class="p">:</span> <span class="mi">1024</span><span class="p">,</span>      <span class="c1"># Context length</span>
        <span class="s2">&quot;emb_dim&quot;</span><span class="p">:</span> <span class="mi">768</span><span class="p">,</span>       <span class="c1"># Embedding dimension</span>
        <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span>        <span class="c1"># Number of attention heads</span>
        <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span>       <span class="c1"># Number of layers</span>
        <span class="s2">&quot;drop_rate&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>     <span class="c1"># Dropout rate</span>
        <span class="s2">&quot;qkv_bias&quot;</span><span class="p">:</span> <span class="kc">False</span>     <span class="c1"># Query-Key-Value bias</span>
    <span class="p">}</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">GPTModel</span><span class="p">(</span><span class="n">GPT_CONFIG_124M</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>  <span class="c1"># disable dropout</span>

    <span class="n">start_context</span> <span class="o">=</span> <span class="s2">&quot;Hello, I am&quot;</span>

    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tiktoken</span><span class="o">.</span><span class="n">get_encoding</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">start_context</span><span class="p">)</span>
    <span class="n">encoded_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="mi">50</span><span class="o">*</span><span class="s1">&#39;=&#39;</span><span class="si">}</span><span class="se">\n</span><span class="si">{</span><span class="mi">22</span><span class="o">*</span><span class="s1">&#39; &#39;</span><span class="si">}</span><span class="s2">IN</span><span class="se">\n</span><span class="si">{</span><span class="mi">50</span><span class="o">*</span><span class="s1">&#39;=&#39;</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Input text:&quot;</span><span class="p">,</span> <span class="n">start_context</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Encoded input text:&quot;</span><span class="p">,</span> <span class="n">encoded</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;encoded_tensor.shape:&quot;</span><span class="p">,</span> <span class="n">encoded_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="n">out</span> <span class="o">=</span> <span class="n">generate_text_simple</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">idx</span><span class="o">=</span><span class="n">encoded_tensor</span><span class="p">,</span>
        <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">context_size</span><span class="o">=</span><span class="n">GPT_CONFIG_124M</span><span class="p">[</span><span class="s2">&quot;ctx_len&quot;</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="n">decoded_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="si">{</span><span class="mi">50</span><span class="o">*</span><span class="s1">&#39;=&#39;</span><span class="si">}</span><span class="se">\n</span><span class="si">{</span><span class="mi">22</span><span class="o">*</span><span class="s1">&#39; &#39;</span><span class="si">}</span><span class="s2">OUT</span><span class="se">\n</span><span class="si">{</span><span class="mi">50</span><span class="o">*</span><span class="s1">&#39;=&#39;</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Output:&quot;</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output length:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output text:&quot;</span><span class="p">,</span> <span class="n">decoded_text</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="n">GPT_CONFIG_124M</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">50257</span><span class="p">,</span>  <span class="c1"># Vocabulary size</span>
    <span class="s2">&quot;ctx_len&quot;</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span>       <span class="c1"># Shortened context length (orig: 1024)</span>
    <span class="s2">&quot;emb_dim&quot;</span><span class="p">:</span> <span class="mi">768</span><span class="p">,</span>       <span class="c1"># Embedding dimension</span>
    <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span>        <span class="c1"># Number of attention heads</span>
    <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span>       <span class="c1"># Number of layers</span>
    <span class="s2">&quot;drop_rate&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>     <span class="c1"># Dropout rate</span>
    <span class="s2">&quot;qkv_bias&quot;</span><span class="p">:</span> <span class="kc">False</span>     <span class="c1"># Query-key-value bias</span>
<span class="p">}</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPTModel</span><span class="p">(</span><span class="n">GPT_CONFIG_124M</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">();</span>  <span class="c1"># Disable dropout during inference</span>
</pre></div>
</div>
</div>
</div>
<p>We use dropout of 0.1 above, but it’s relatively common to train LLMs without dropout nowadays
Modern LLMs also don’t use bias vectors in the <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code> layers for the query, key, and value matrices (unlike earlier GPT models), which is achieved by setting <code class="docutils literal notranslate"><span class="pre">&quot;qkv_bias&quot;:</span> <span class="pre">False</span></code>
We reduce the context length (<code class="docutils literal notranslate"><span class="pre">ctx_len</span></code>) of only 256 tokens to reduce the computational resource requirements for training the model, whereas the original 124 million parameter GPT-2 model used 1024 characters
This is so that more readers will be able to follow and execute the code examples on their laptop computer
However, please feel free to increase the <code class="docutils literal notranslate"><span class="pre">ctx_len</span></code> to 1024 tokens (this would not require any code changes)
We will also load a model with a 1024 <code class="docutils literal notranslate"><span class="pre">ctx_len</span></code> later from pretrained weights</p>
<p>Next, we use the <code class="docutils literal notranslate"><span class="pre">generate_text_simple</span></code> function from the previous chapter to generate text
In addition, we define two convenience functions, <code class="docutils literal notranslate"><span class="pre">text_to_token_ids</span></code> and <code class="docutils literal notranslate"><span class="pre">token_ids_to_text</span></code>, for converting between token and text representations that we use throughout this chapter</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">tiktoken</span>

<span class="k">def</span><span class="w"> </span><span class="nf">text_to_token_ids</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">allowed_special</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;&lt;|endoftext|&gt;&#39;</span><span class="p">})</span>
    <span class="n">encoded_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># add batch dimension</span>
    <span class="k">return</span> <span class="n">encoded_tensor</span>

<span class="k">def</span><span class="w"> </span><span class="nf">token_ids_to_text</span><span class="p">(</span><span class="n">token_ids</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
    <span class="n">flat</span> <span class="o">=</span> <span class="n">token_ids</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># remove batch dimension</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">flat</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>

<span class="n">start_context</span> <span class="o">=</span> <span class="s2">&quot;Every effort moves you&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tiktoken</span><span class="o">.</span><span class="n">get_encoding</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>

<span class="n">token_ids</span> <span class="o">=</span> <span class="n">generate_text_simple</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">idx</span><span class="o">=</span><span class="n">text_to_token_ids</span><span class="p">(</span><span class="n">start_context</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">),</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">context_size</span><span class="o">=</span><span class="n">GPT_CONFIG_124M</span><span class="p">[</span><span class="s2">&quot;ctx_len&quot;</span><span class="p">]</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output text:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">token_ids_to_text</span><span class="p">(</span><span class="n">token_ids</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Output text:
 Every effort moves you rentingetic wasnم refres RexMeCHicular stren
</pre></div>
</div>
</div>
</div>
<p>As we can see above, the model does not produce good text because it has not been trained yet
How do we measure or capture what “good text” is, in a numeric form, to track it during training?
The next subsection introduces metrics to calculate a loss metric for the generated outputs that we can use to measure the training progress
The next chapters on finetuning LLMs will also introduce additional ways to measure model quality</p>
<br></section>
<section id="calculating-the-text-generation-loss-cross-entropy-and-perplexity">
<h3><span class="section-number">6.1.2. </span>Calculating the text generation loss: cross entropy, and perplexity<a class="headerlink" href="#calculating-the-text-generation-loss-cross-entropy-and-perplexity" title="Permalink to this headline">#</a></h3>
<p>Suppose we have an <code class="docutils literal notranslate"><span class="pre">inputs</span></code> tensor containing the token IDs for 2 training examples (rows)
Corresponding to the <code class="docutils literal notranslate"><span class="pre">inputs</span></code>, the <code class="docutils literal notranslate"><span class="pre">targets</span></code> contain the desired token IDs that we want the model to generate
Notice that the <code class="docutils literal notranslate"><span class="pre">targets</span></code> are the <code class="docutils literal notranslate"><span class="pre">inputs</span></code> shifted by 1 position, as explained in chapter 2 when we implemented the data loader</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">16833</span><span class="p">,</span> <span class="mi">3626</span><span class="p">,</span> <span class="mi">6100</span><span class="p">],</span>   <span class="c1"># [&quot;every effort moves&quot;,</span>
                       <span class="p">[</span><span class="mi">40</span><span class="p">,</span>    <span class="mi">1107</span><span class="p">,</span> <span class="mi">588</span><span class="p">]])</span>   <span class="c1">#  &quot;I really like&quot;]</span>

<span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">3626</span><span class="p">,</span> <span class="mi">6100</span><span class="p">,</span> <span class="mi">345</span>  <span class="p">],</span>  <span class="c1"># [&quot; effort moves you&quot;,</span>
                        <span class="p">[</span><span class="mi">588</span><span class="p">,</span>  <span class="mi">428</span><span class="p">,</span>  <span class="mi">11311</span><span class="p">]])</span> <span class="c1">#  &quot; really like chocolate&quot;]</span>
</pre></div>
</div>
</div>
</div>
<p>Feeding the <code class="docutils literal notranslate"><span class="pre">inputs</span></code> to the model, we obtain the logits vector for the 2 input examples that consist of 3 tokens each
Each of the tokens is a 50,257-dimensional vector corresponding to the size of the vocabulary
Applying the softmax function, we can turn the logits tensor into a tensor of the same dimension containing probability scores</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

<span class="n">probas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Probability of each token in vocabulary</span>
<span class="nb">print</span><span class="p">(</span><span class="n">probas</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># Shape: (batch_size, num_tokens, vocab_size)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([2, 3, 50257])
</pre></div>
</div>
</div>
</div>
<p>The figure below, using a very small vocabulary for illustration purposes, outlines how we convert the probability scores back into text, which we discussed at the end of the previous chapter</p>
<p>As discussed in the previous chapter, we can apply the <code class="docutils literal notranslate"><span class="pre">argmax</span></code> function to convert the probability scores into predicted token IDs
The softmax function above produced a 50,257-dimensional vector for each token; the <code class="docutils literal notranslate"><span class="pre">argmax</span></code> function returns the position of the highest probability score in this vector, which is the predicted token ID for the given token</p>
<p>Since we have 2 input batches with 3 tokens each, we obtain 2 by 3 predicted token IDs:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">token_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">probas</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Token IDs:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">token_ids</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Token IDs:
 tensor([[[16657],
         [  339],
         [42826]],

        [[49906],
         [29669],
         [41751]]])
</pre></div>
</div>
</div>
</div>
<p>If we decode these tokens, we find that these are quite different from the tokens we want the model to predict, namely the target tokens:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Targets batch 1: </span><span class="si">{</span><span class="n">token_ids_to_text</span><span class="p">(</span><span class="n">targets</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="n">tokenizer</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Outputs batch 1: </span><span class="si">{</span><span class="n">token_ids_to_text</span><span class="p">(</span><span class="n">token_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span><span class="w"> </span><span class="n">tokenizer</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Targets batch 1:  effort moves you
Outputs batch 1:  Armed heNetflix
</pre></div>
</div>
</div>
</div>
<p>That’s because the model wasn’t trained yet
To train the model, we need to know how far it is away from the correct predictions (targets)</p>
<p>The token probabilities corresponding to the target indices are as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">batch_idx</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">target_probas_1</span> <span class="o">=</span> <span class="n">probas</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">targets</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Batch 1:&quot;</span><span class="p">,</span> <span class="n">target_probas_1</span><span class="p">)</span>

<span class="n">batch_idx</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">target_probas_2</span> <span class="o">=</span> <span class="n">probas</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">targets</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Batch 2:&quot;</span><span class="p">,</span> <span class="n">target_probas_2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Batch 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])
Batch 2: tensor([3.9836e-05, 1.6783e-05, 4.7559e-06])
</pre></div>
</div>
</div>
</div>
<p>We want to maximize all these values, bringing them close to a probability of 1
In mathematical optimization, it is easier to maximize the logarithm of the probability score than the probability score itself</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute logarithm of all token probabilities</span>
<span class="n">log_probas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">target_probas_1</span><span class="p">,</span> <span class="n">target_probas_2</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">log_probas</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([ -9.5042, -10.3796, -11.3677, -10.1308, -10.9951, -12.2561])
</pre></div>
</div>
</div>
</div>
<p>Next, we compute the average log probability:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate the average probability for each token</span>
<span class="n">avg_log_probas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">log_probas</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">avg_log_probas</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(-10.7722)
</pre></div>
</div>
</div>
</div>
<p>The goal is to make this average log probability as large as possible by optimizing the model weights
Due to the log, the largest possible value is 0, and we are currently far away from 0</p>
<p>In deep learning, instead of maximizing the average log-probability, it’s a standard convention to minimize the <em>negative</em> average log-probability value; in our case, instead of maximizing -10.7722 so that it approaches 0, in deep learning, we would minimize 10.7722 so that it approaches 0
The value negative of -10.7722, i.e., 10.7722, is also called cross entropy loss in deep learning</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">neg_avg_log_probas</span> <span class="o">=</span> <span class="n">avg_log_probas</span> <span class="o">*</span> <span class="o">-</span><span class="mi">1</span>
<span class="nb">print</span><span class="p">(</span><span class="n">neg_avg_log_probas</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(10.7722)
</pre></div>
</div>
</div>
</div>
<p>PyTorch already implements a <code class="docutils literal notranslate"><span class="pre">cross_entropy</span></code> function that carries out the previous steps</p>
<p>Before we apply the cross entropy function, let’s check the shape of the logits and targets</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Logits have shape (batch_size, num_tokens, vocab_size)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Logits shape:&quot;</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Targets have shape (batch_size, num_tokens)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Targets shape:&quot;</span><span class="p">,</span> <span class="n">targets</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Logits shape: torch.Size([2, 3, 50257])
Targets shape: torch.Size([2, 3])
</pre></div>
</div>
</div>
</div>
<p>For the cross <code class="docutils literal notranslate"><span class="pre">entropy_loss</span></code> function in PyTorch, we want to flatten these tensors by combining them over the batch dimension:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logits_flat</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">targets_flat</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Flattened logits:&quot;</span><span class="p">,</span> <span class="n">logits_flat</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Flattened targets:&quot;</span><span class="p">,</span> <span class="n">targets_flat</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Flattened logits: torch.Size([6, 50257])
Flattened targets: torch.Size([6])
</pre></div>
</div>
</div>
</div>
<p>Note that the targets are the token IDs, which also represent the index positions in the logits tensors that we want to maximize
The <code class="docutils literal notranslate"><span class="pre">cross_entropy</span></code> function in PyTorch will automatically take care of applying the softmax and log-probability computation internally over those token indices in the logits that are to be maximized</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits_flat</span><span class="p">,</span> <span class="n">targets_flat</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(10.7722)
</pre></div>
</div>
</div>
</div>
<p>A concept related to the cross entropy loss is the perplexity of an LLM
The perplexity is simply the exponential of the cross entropy loss</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">perplexity</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">perplexity</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(47678.8633)
</pre></div>
</div>
</div>
</div>
<p>The perplexity is often considered more interpretable because it can be understood as the effective vocabulary size that the model is uncertain about at each step (in the example above, that’d be 47,678 words or tokens)
In other words, perplexity provides a measure of how well the probability distribution predicted by the model matches the actual distribution of the words in the dataset
Similar to the loss, a lower perplexity indicates that the model predictions are closer to the actual distribution</p>
</section>
<section id="calculating-the-training-and-validation-set-losses">
<h3><span class="section-number">6.1.3. </span>Calculating the training and validation set losses<a class="headerlink" href="#calculating-the-training-and-validation-set-losses" title="Permalink to this headline">#</a></h3>
<p>We use a relatively small dataset for training the LLM (in fact, only one short story)
The reasons are:
You can run the code examples in a few minutes on a laptop computer without a suitable GPU
The training finishes relatively fast (minutes instead of weeks), which is good for educational purposes
We use a text from the public domain, which can be included in this GitHub repository without violating any usage rights or bloating the repository size</p>
<p>For example, Llama 2 7B required 184,320 GPU hours on A100 GPUs to be trained on 2 trillion tokens
At the time of this writing, the hourly cost of an 8xA100 cloud server at AWS is approximately \<span class="math notranslate nohighlight">\(30
  So, via an off-the-envelope calculation, training this LLM would cost 184,320 / 8 * \\\)</span>30 =  \$690,000</p>
<p>Below, we use the same dataset we used in chapter 2</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">urllib.request</span>

<span class="n">file_path</span> <span class="o">=</span> <span class="s2">&quot;the-verdict.txt&quot;</span>
<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/llm/the-verdict.txt&quot;</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">file_path</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">urllib</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">urlopen</span><span class="p">(</span><span class="n">url</span><span class="p">)</span> <span class="k">as</span> <span class="n">response</span><span class="p">:</span>
        <span class="n">text_data</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
        <span class="n">file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">text_data</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
        <span class="n">text_data</span> <span class="o">=</span> <span class="n">file</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>A quick check that the text loaded ok by printing the first and last 100 words</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># First 100 characters</span>
<span class="nb">print</span><span class="p">(</span><span class="n">text_data</span><span class="p">[:</span><span class="mi">99</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Last 100 characters</span>
<span class="nb">print</span><span class="p">(</span><span class="n">text_data</span><span class="p">[</span><span class="o">-</span><span class="mi">99</span><span class="p">:])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>it for me! The Strouds stand alone, and happen once--but there&#39;s no exterminating our kind of art.&quot;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">total_char</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">text_data</span><span class="p">)</span>
<span class="n">total_tokens</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text_data</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Characters:&quot;</span><span class="p">,</span> <span class="n">total_char</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tokens:&quot;</span><span class="p">,</span> <span class="n">total_tokens</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Characters: 20479
Tokens: 5145
</pre></div>
</div>
</div>
</div>
<p>With 5,145 tokens, the text is very short for training an LLM, but again, it’s for educational purposes (we will also load pretrained weights later)</p>
<p>Next, we divide the dataset into a training and a validation set and use the data loaders from chapter 2 to prepare the batches for LLM training
For visualization purposes, the figure below assumes a <code class="docutils literal notranslate"><span class="pre">max_length=6</span></code>, but for the training loader, we set the <code class="docutils literal notranslate"><span class="pre">max_length</span></code> equal to the context length that the LLM supports
The figure below only shows the input tokens for simplicity
Since we train the LLM to predict the next word in the text, the targets look the same as these inputs, except that the targets are shifted by one position</p>
<img src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/llm/clipboard_20240409_035413.png" width=500px><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Train/validation ratio</span>
<span class="n">train_ratio</span> <span class="o">=</span> <span class="mf">0.90</span>
<span class="n">split_idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">train_ratio</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">text_data</span><span class="p">))</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">text_data</span><span class="p">[:</span><span class="n">split_idx</span><span class="p">]</span>
<span class="n">val_data</span> <span class="o">=</span> <span class="n">text_data</span><span class="p">[</span><span class="n">split_idx</span><span class="p">:]</span>


<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">create_dataloader_v1</span><span class="p">(</span>
    <span class="n">train_data</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=</span><span class="n">GPT_CONFIG_124M</span><span class="p">[</span><span class="s2">&quot;ctx_len&quot;</span><span class="p">],</span>
    <span class="n">stride</span><span class="o">=</span><span class="n">GPT_CONFIG_124M</span><span class="p">[</span><span class="s2">&quot;ctx_len&quot;</span><span class="p">],</span>
    <span class="n">drop_last</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="n">val_loader</span> <span class="o">=</span> <span class="n">create_dataloader_v1</span><span class="p">(</span>
    <span class="n">val_data</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=</span><span class="n">GPT_CONFIG_124M</span><span class="p">[</span><span class="s2">&quot;ctx_len&quot;</span><span class="p">],</span>
    <span class="n">stride</span><span class="o">=</span><span class="n">GPT_CONFIG_124M</span><span class="p">[</span><span class="s2">&quot;ctx_len&quot;</span><span class="p">],</span>
    <span class="n">drop_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Sanity check</span>

<span class="k">if</span> <span class="n">total_tokens</span> <span class="o">*</span> <span class="p">(</span><span class="n">train_ratio</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">GPT_CONFIG_124M</span><span class="p">[</span><span class="s2">&quot;ctx_len&quot;</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Not enough tokens for the training loader. &quot;</span>
          <span class="s2">&quot;Try to lower the `GPT_CONFIG_124M[&#39;ctx_len&#39;]` or &quot;</span>
          <span class="s2">&quot;increase the `training_ratio`&quot;</span><span class="p">)</span>

<span class="k">if</span> <span class="n">total_tokens</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">train_ratio</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">GPT_CONFIG_124M</span><span class="p">[</span><span class="s2">&quot;ctx_len&quot;</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Not enough tokens for the validation loader. &quot;</span>
          <span class="s2">&quot;Try to lower the `GPT_CONFIG_124M[&#39;ctx_len&#39;]` or &quot;</span>
          <span class="s2">&quot;decrease the `training_ratio`&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We use a relatively small batch size to reduce the computational resource demand, and because the dataset is very small to begin with
Llama 2 7B was trained with a batch size of 1024, for example</p>
<p>An optional check that the data was loaded correctly:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train loader:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Validation loader:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">val_loader</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train loader:
torch.Size([2, 256]) torch.Size([2, 256])
torch.Size([2, 256]) torch.Size([2, 256])
torch.Size([2, 256]) torch.Size([2, 256])
torch.Size([2, 256]) torch.Size([2, 256])
torch.Size([2, 256]) torch.Size([2, 256])
torch.Size([2, 256]) torch.Size([2, 256])
torch.Size([2, 256]) torch.Size([2, 256])
torch.Size([2, 256]) torch.Size([2, 256])
torch.Size([2, 256]) torch.Size([2, 256])

Validation loader:
torch.Size([2, 256]) torch.Size([2, 256])
</pre></div>
</div>
</div>
</div>
<p>Another optional check that the token sizes are in the expected ballpark:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_tokens</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">input_batch</span><span class="p">,</span> <span class="n">target_batch</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
    <span class="n">train_tokens</span> <span class="o">+=</span> <span class="n">input_batch</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>

<span class="n">val_tokens</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">input_batch</span><span class="p">,</span> <span class="n">target_batch</span> <span class="ow">in</span> <span class="n">val_loader</span><span class="p">:</span>
    <span class="n">val_tokens</span> <span class="o">+=</span> <span class="n">input_batch</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training tokens:&quot;</span><span class="p">,</span> <span class="n">train_tokens</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Validation tokens:&quot;</span><span class="p">,</span> <span class="n">val_tokens</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;All tokens:&quot;</span><span class="p">,</span> <span class="n">train_tokens</span> <span class="o">+</span> <span class="n">val_tokens</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training tokens: 4608
Validation tokens: 512
All tokens: 5120
</pre></div>
</div>
</div>
</div>
<p>Next, we implement a utility function to calculate the cross entropy loss of a given batch
In addition, we implement a second utility function to compute the loss for a user-specified number of batches in a data loader</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">calc_loss_batch</span><span class="p">(</span><span class="n">input_batch</span><span class="p">,</span> <span class="n">target_batch</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
    <span class="n">input_batch</span><span class="p">,</span> <span class="n">target_batch</span> <span class="o">=</span> <span class="n">input_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">target_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_batch</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">target_batch</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">loss</span>


<span class="k">def</span><span class="w"> </span><span class="nf">calc_loss_loader</span><span class="p">(</span><span class="n">data_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">num_batches</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="k">if</span> <span class="n">num_batches</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">num_batches</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Reduce the number of batches to match the total number of batches in the data loader</span>
        <span class="c1"># if num_batches exceeds the number of batches in the data loader</span>
        <span class="n">num_batches</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">num_batches</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_loader</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">input_batch</span><span class="p">,</span> <span class="n">target_batch</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data_loader</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_batches</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">calc_loss_batch</span><span class="p">(</span><span class="n">input_batch</span><span class="p">,</span> <span class="n">target_batch</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
            <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="n">num_batches</span>
</pre></div>
</div>
</div>
</div>
<p>If you have a machine with a CUDA-supported GPU, the LLM will train on the GPU without making any changes to the code
Via the <code class="docutils literal notranslate"><span class="pre">device</span></code> setting, we ensure that the data is loaded onto the same device as the LLM model</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="c1"># no assignment model = model.to(device) necessary for nn.Module classes</span>


<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span> <span class="c1"># For reproducibility due to the shuffling in the data loader</span>
<span class="n">train_loss</span> <span class="o">=</span> <span class="n">calc_loss_loader</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
<span class="n">val_loss</span> <span class="o">=</span> <span class="n">calc_loss_loader</span><span class="p">(</span><span class="n">val_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training loss:&quot;</span><span class="p">,</span> <span class="n">train_loss</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Validation loss:&quot;</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training loss: 10.98758347829183
Validation loss: 10.98110580444336
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="training-an-llm">
<h2><span class="section-number">6.2. </span>Training an LLM<a class="headerlink" href="#training-an-llm" title="Permalink to this headline">#</a></h2>
<p>In this section, we finally implement the code for training the LLM ( We focus on a simple training function )</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">train_model_simple</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span>
                       <span class="n">eval_freq</span><span class="p">,</span> <span class="n">eval_iter</span><span class="p">,</span> <span class="n">start_context</span><span class="p">):</span>
    <span class="c1"># Initialize lists to track losses and tokens seen</span>
    <span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span><span class="p">,</span> <span class="n">track_tokens_seen</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="n">tokens_seen</span><span class="p">,</span> <span class="n">global_step</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span>

    <span class="c1"># Main training loop</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  <span class="c1"># Set model to training mode</span>
        
        <span class="k">for</span> <span class="n">input_batch</span><span class="p">,</span> <span class="n">target_batch</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span> <span class="c1"># Reset loss gradients from previous epoch</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">calc_loss_batch</span><span class="p">(</span><span class="n">input_batch</span><span class="p">,</span> <span class="n">target_batch</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># Calculate loss gradients</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span> <span class="c1"># Update model weights using loss gradients</span>
            <span class="n">tokens_seen</span> <span class="o">+=</span> <span class="n">input_batch</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
            <span class="n">global_step</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="c1"># Optional evaluation step</span>
            <span class="k">if</span> <span class="n">global_step</span> <span class="o">%</span> <span class="n">eval_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">train_loss</span><span class="p">,</span> <span class="n">val_loss</span> <span class="o">=</span> <span class="n">evaluate_model</span><span class="p">(</span>
                    <span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">eval_iter</span><span class="p">)</span>
                <span class="n">train_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_loss</span><span class="p">)</span>
                <span class="n">val_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val_loss</span><span class="p">)</span>
                <span class="n">track_tokens_seen</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tokens_seen</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Ep </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> (Step </span><span class="si">{</span><span class="n">global_step</span><span class="si">:</span><span class="s2">06d</span><span class="si">}</span><span class="s2">): &quot;</span>
                      <span class="sa">f</span><span class="s2">&quot;Train loss </span><span class="si">{</span><span class="n">train_loss</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, Val loss </span><span class="si">{</span><span class="n">val_loss</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># Print a sample text after each epoch</span>
        <span class="n">generate_and_print_sample</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">start_context</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span><span class="p">,</span> <span class="n">track_tokens_seen</span>


<span class="k">def</span><span class="w"> </span><span class="nf">evaluate_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">eval_iter</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">train_loss</span> <span class="o">=</span> <span class="n">calc_loss_loader</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">num_batches</span><span class="o">=</span><span class="n">eval_iter</span><span class="p">)</span>
        <span class="n">val_loss</span> <span class="o">=</span> <span class="n">calc_loss_loader</span><span class="p">(</span><span class="n">val_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">num_batches</span><span class="o">=</span><span class="n">eval_iter</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">train_loss</span><span class="p">,</span> <span class="n">val_loss</span>


<span class="k">def</span><span class="w"> </span><span class="nf">generate_and_print_sample</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">start_context</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">context_size</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">pos_emb</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="n">text_to_token_ids</span><span class="p">(</span><span class="n">start_context</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">token_ids</span> <span class="o">=</span> <span class="n">generate_text_simple</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">idx</span><span class="o">=</span><span class="n">encoded</span><span class="p">,</span>
            <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">context_size</span><span class="o">=</span><span class="n">context_size</span>
        <span class="p">)</span>
        <span class="n">decoded_text</span> <span class="o">=</span> <span class="n">token_ids_to_text</span><span class="p">(</span><span class="n">token_ids</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">decoded_text</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">))</span>  <span class="c1"># Compact print format</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Now, let’s train the LLM using the training function defined above:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPTModel</span><span class="p">(</span><span class="n">GPT_CONFIG_124M</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span><span class="p">,</span> <span class="n">tokens_seen</span> <span class="o">=</span> <span class="n">train_model_simple</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span>
    <span class="n">num_epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">eval_freq</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">eval_iter</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">start_context</span><span class="o">=</span><span class="s2">&quot;Every effort moves you&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Ep 1 (Step 000000): Train loss 9.558, Val loss 9.856
Ep 1 (Step 000005): Train loss 7.651, Val loss 8.051
Every effort moves you,,,,,,,,,,,,.                                     
Ep 2 (Step 000010): Train loss 6.421, Val loss 6.812
Ep 2 (Step 000015): Train loss 5.913, Val loss 6.559
Every effort moves you, and, and,, and,,,,, and, and,,,, and,,,, and, and,, and,,,, and,, and,,,,, and,,,,,,
Ep 3 (Step 000020): Train loss 5.680, Val loss 6.490
Ep 3 (Step 000025): Train loss 5.557, Val loss 6.602
Every effort moves you, and, and the picture.                             &quot;, and, and the, and the, and, and,
Ep 4 (Step 000030): Train loss 5.204, Val loss 6.508
Ep 4 (Step 000035): Train loss 4.865, Val loss 6.420
Every effort moves you, and I had a a a--I to the picture. &quot;I. I had the picture. &quot;I had the picture. I had the the picture. I had the the picture. &quot;I had the picture. &quot;
Ep 5 (Step 000040): Train loss 4.332, Val loss 6.328
Every effort moves you, I was a and I was a little to the picture.                                     
Ep 6 (Step 000045): Train loss 4.295, Val loss 6.221
Ep 6 (Step 000050): Train loss 3.268, Val loss 6.184
Every effort moves you know to see the end of the Riv I felt--the a little of the last: &quot;                               
Ep 7 (Step 000055): Train loss 2.880, Val loss 6.129
Ep 7 (Step 000060): Train loss 2.820, Val loss 6.194
Every effort moves you know the fact of a little a--I was his painting.                                     
Ep 8 (Step 000065): Train loss 2.260, Val loss 6.236
Ep 8 (Step 000070): Train loss 1.754, Val loss 6.260
Every effort moves you know,&quot; was one of the picture for nothing--I turned Mrs.                                    
Ep 9 (Step 000075): Train loss 1.447, Val loss 6.319
Ep 9 (Step 000080): Train loss 1.120, Val loss 6.310
Every effort moves you?&quot;               &quot;I looked--and me.&quot;         He placed them at my elbow and I looked up his pictures--because he&#39;s I had
Ep 10 (Step 000085): Train loss 0.762, Val loss 6.372
Every effort moves you?&quot;  &quot;Yes--quite insensible to the irony. She wanted him vindicated--and by me!&quot;  He laughed again, and threw back his head to look up at the sketch of the donkey. &quot;There were days when I
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="k">def</span><span class="w"> </span><span class="nf">plot_losses</span><span class="p">(</span><span class="n">epochs_seen</span><span class="p">,</span> <span class="n">tokens_seen</span><span class="p">,</span> <span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

    <span class="c1"># Plot training and validation loss against epochs</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs_seen</span><span class="p">,</span> <span class="n">train_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Training loss&quot;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs_seen</span><span class="p">,</span> <span class="n">val_losses</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;-.&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Validation loss&quot;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Epochs&quot;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">)</span>

    <span class="c1"># Create a second x-axis for tokens seen</span>
    <span class="n">ax2</span> <span class="o">=</span> <span class="n">ax1</span><span class="o">.</span><span class="n">twiny</span><span class="p">()</span>  <span class="c1"># Create a second x-axis that shares the same y-axis</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">tokens_seen</span><span class="p">,</span> <span class="n">train_losses</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Invisible plot for aligning ticks</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Tokens seen&quot;</span><span class="p">)</span>

    <span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>  <span class="c1"># Adjust layout to make room</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s2">&quot;loss-plot.pdf&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">epochs_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_losses</span><span class="p">))</span>
<span class="n">plot_losses</span><span class="p">(</span><span class="n">epochs_tensor</span><span class="p">,</span> <span class="n">tokens_seen</span><span class="p">,</span> <span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/pretraining-on-unlabeled-data_73_0.png" src="../_images/pretraining-on-unlabeled-data_73_0.png" />
</div>
</div>
<p>Looking at the results above, we can see that the model starts out generating incomprehensible strings of words, whereas towards the end, it’s able to produce grammatically more or less correct sentences
However, based on the training and validation set losses, we can see that the model starts overfitting
If we were to check a few passages it writes towards the end, we would find that they are contained in the training set verbatim -it simply memorizes the training data
Later, we will cover decoding strategies that can mitigate this memorization by a certain degree
Note that the overfitting here occurs because we have a very, very small training set, and we iterate over it so many times
The LLM training here primarily serves educational purposes; we mainly want to see that the model can learn to produce coherent text
Instead of spending weeks or months on training this model on vast amounts of expensive hardware, we load pretrained weights later</p>
</section>
<section id="decoding-strategies-to-control-randomness">
<h2><span class="section-number">6.3. </span>Decoding strategies to control randomness<a class="headerlink" href="#decoding-strategies-to-control-randomness" title="Permalink to this headline">#</a></h2>
<p>Inference is relatively cheap with a relatively small LLM as the GPT model we trained above, so there’s no need to use a GPU for it in case you used a GPU for training it above
Using the <code class="docutils literal notranslate"><span class="pre">generate_text_simple</span></code> function (from the previous chapter) that we used earlier inside the simple training function, we can generate new text one word (or token) at a time
As explained in sections above, the next generated token is the token corresponding to the largest probability score among all tokens in the vocabulary</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tiktoken</span><span class="o">.</span><span class="n">get_encoding</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>

<span class="n">token_ids</span> <span class="o">=</span> <span class="n">generate_text_simple</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">idx</span><span class="o">=</span><span class="n">text_to_token_ids</span><span class="p">(</span><span class="s2">&quot;Every effort moves you&quot;</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">),</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
    <span class="n">context_size</span><span class="o">=</span><span class="n">GPT_CONFIG_124M</span><span class="p">[</span><span class="s2">&quot;ctx_len&quot;</span><span class="p">]</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output text:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">token_ids_to_text</span><span class="p">(</span><span class="n">token_ids</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Output text:
 Every effort moves you?&quot;

&quot;Yes--quite insensible to the irony. She wanted him vindicated--and by me!&quot;
</pre></div>
</div>
</div>
</div>
<p>Even if we execute the <code class="docutils literal notranslate"><span class="pre">generate_text_simple</span></code> function above multiple times, the LLM will always generate the same outputs
We now introduce two concepts, so-called decoding strategies, to modify the <code class="docutils literal notranslate"><span class="pre">generate_text_simple</span></code>: <em>temperature scaling</em> and <em>top-k</em> sampling
These will allow the model to control the randomness and diversity of the generated text</p>
<section id="temperature-scaling">
<h3><span class="section-number">6.3.1. </span>Temperature scaling<a class="headerlink" href="#temperature-scaling" title="Permalink to this headline">#</a></h3>
<p>Previously, we always sampled the token with the highest probability as the next token using <code class="docutils literal notranslate"><span class="pre">torch.argmax</span></code>
To add variety, we can sample the next token using The <code class="docutils literal notranslate"><span class="pre">torch.multinomial(probs,</span> <span class="pre">num_samples=1)</span></code>, sampling from a probability distribution
Here, each index’s chance of being picked corresponds to its probability in the input tensor</p>
<p>Here’s a little recap of generating the next token, assuming a very small vocabulary for illustration purposes:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vocab</span> <span class="o">=</span> <span class="p">{</span> 
    <span class="s2">&quot;closer&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s2">&quot;every&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> 
    <span class="s2">&quot;effort&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> 
    <span class="s2">&quot;forward&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="s2">&quot;inches&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
    <span class="s2">&quot;moves&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span> 
    <span class="s2">&quot;pizza&quot;</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>
    <span class="s2">&quot;toward&quot;</span><span class="p">:</span> <span class="mi">7</span><span class="p">,</span>
    <span class="s2">&quot;you&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
<span class="p">}</span> 

<span class="n">inverse_vocab</span> <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">vocab</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

<span class="c1"># Suppose input is &quot;every effort moves you&quot;, and the LLM</span>
<span class="c1"># returns the following logits for the next token:</span>
<span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
    <span class="p">[</span><span class="mf">4.51</span><span class="p">,</span> <span class="mf">0.89</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.90</span><span class="p">,</span> <span class="mf">6.75</span><span class="p">,</span> <span class="mf">1.63</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.62</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.89</span><span class="p">,</span> <span class="mf">6.28</span><span class="p">,</span> <span class="mf">1.79</span><span class="p">]</span>
<span class="p">)</span>

<span class="n">probas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">next_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">next_token_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">probas</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

<span class="c1"># The next generated token is then as follows:</span>
<span class="nb">print</span><span class="p">(</span><span class="n">inverse_vocab</span><span class="p">[</span><span class="n">next_token_id</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>forward
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">next_token_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probas</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">inverse_vocab</span><span class="p">[</span><span class="n">next_token_id</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>forward
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">print_sampled_tokens</span><span class="p">(</span><span class="n">probas</span><span class="p">):</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span> <span class="c1"># Manual seed for reproducibility</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probas</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1_000</span><span class="p">)]</span>
    <span class="n">sampled_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">sample</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">freq</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sampled_ids</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">freq</span><span class="si">}</span><span class="s2"> x </span><span class="si">{</span><span class="n">inverse_vocab</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">print_sampled_tokens</span><span class="p">(</span><span class="n">probas</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>73 x closer
0 x every
0 x effort
582 x forward
2 x inches
0 x moves
0 x pizza
343 x toward
</pre></div>
</div>
</div>
</div>
<p>Instead of determining the most likely token via <code class="docutils literal notranslate"><span class="pre">torch.argmax</span></code>, we use <code class="docutils literal notranslate"><span class="pre">torch.multinomial(probas,</span> <span class="pre">num_samples=1)</span></code> to determine the most likely token by sampling from the softmax distribution
For illustration purposes, let’s see what happens when we sample the next token 1,000 times using the original softmax probabilities:</p>
<p>We can control the distribution and selection process via a concept called temperature scaling
“Temperature scaling” is just a fancy word for dividing the logits by a number greater than 0
Temperatures greater than 1 will result in more uniformly distributed token probabilities after applying the softmax
Temperatures smaller than 1 will result in more confident (sharper or more peaky) distributions after applying the softmax</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">softmax_with_temperature</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">temperature</span><span class="p">):</span>
    <span class="n">scaled_logits</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">/</span> <span class="n">temperature</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scaled_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Temperature values</span>
<span class="n">temperatures</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>  <span class="c1"># Original, higher confidence, and lower confidence</span>

<span class="c1"># Calculate scaled probabilities</span>
<span class="n">scaled_probas</span> <span class="o">=</span> <span class="p">[</span><span class="n">softmax_with_temperature</span><span class="p">(</span><span class="n">next_token_logits</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span> <span class="k">for</span> <span class="n">T</span> <span class="ow">in</span> <span class="n">temperatures</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plotting</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">))</span>
<span class="n">bar_width</span> <span class="o">=</span> <span class="mf">0.15</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">T</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">temperatures</span><span class="p">):</span>
    <span class="n">rects</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">bar_width</span><span class="p">,</span> <span class="n">scaled_probas</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">bar_width</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Temperature = </span><span class="si">{</span><span class="n">T</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Probability&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">vocab</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s2">&quot;temperature-plot.pdf&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/pretraining-on-unlabeled-data_88_0.png" src="../_images/pretraining-on-unlabeled-data_88_0.png" />
</div>
</div>
<p>We can see that the rescaling via temperature 0.1 results in a sharper distribution, approaching <code class="docutils literal notranslate"><span class="pre">torch.argmax</span></code>, such that the most likely word is almost always selected:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">print_sampled_tokens</span><span class="p">(</span><span class="n">scaled_probas</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0 x closer
0 x every
0 x effort
985 x forward
0 x inches
0 x moves
0 x pizza
15 x toward
</pre></div>
</div>
</div>
</div>
<p>The rescaled probabilities via temperature 5 are more uniformly distributed:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">print_sampled_tokens</span><span class="p">(</span><span class="n">scaled_probas</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>165 x closer
75 x every
42 x effort
239 x forward
71 x inches
46 x moves
32 x pizza
227 x toward
103 x you
</pre></div>
</div>
</div>
</div>
<p>Assuming an LLM input “every effort moves you”, using the approach above can sometimes result in nonsensical texts, such as “every effort moves you pizza”, 3.2% of the time (32 out of 1000 times)</p>
</section>
<section id="top-k-sampling">
<h3><span class="section-number">6.3.2. </span>Top-k sampling<a class="headerlink" href="#top-k-sampling" title="Permalink to this headline">#</a></h3>
<p>To be able to use higher temperatures to increase output diversity and to reduce the probability of nonsensical sentences, we can restrict the sampled tokens to the top-k most likely tokens:</p>
<p>In code, we can implement this as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">top_k</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">top_logits</span><span class="p">,</span> <span class="n">top_pos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">next_token_logits</span><span class="p">,</span> <span class="n">top_k</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Top logits:&quot;</span><span class="p">,</span> <span class="n">top_logits</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Top positions:&quot;</span><span class="p">,</span> <span class="n">top_pos</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Top logits: tensor([6.7500, 6.2800, 4.5100])
Top positions: tensor([3, 7, 0])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">new_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
    <span class="n">condition</span><span class="o">=</span><span class="n">next_token_logits</span> <span class="o">&lt;</span> <span class="n">top_logits</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
    <span class="nb">input</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">)),</span> 
    <span class="n">other</span><span class="o">=</span><span class="n">next_token_logits</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">new_logits</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">topk_probas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">new_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">topk_probas</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])
</pre></div>
</div>
</div>
</div>
</section>
<section id="modifying-the-text-generation-function">
<h3><span class="section-number">6.3.3. </span>Modifying the text generation function<a class="headerlink" href="#modifying-the-text-generation-function" title="Permalink to this headline">#</a></h3>
<p>The previous two subsections introduced temperature sampling and top-k sampling
Let’s use these two concepts to modify the <code class="docutils literal notranslate"><span class="pre">generate_simple</span></code> function we used to generate text via the LLM earlier, creating a new <code class="docutils literal notranslate"><span class="pre">generate</span></code> function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="p">,</span> <span class="n">context_size</span><span class="p">,</span> <span class="n">temperature</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

    <span class="c1"># For-loop is the same as before: Get logits, and only focus on last time step</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="p">):</span>
        <span class="n">idx_cond</span> <span class="o">=</span> <span class="n">idx</span><span class="p">[:,</span> <span class="o">-</span><span class="n">context_size</span><span class="p">:]</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">idx_cond</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>

        <span class="c1"># New: Filter logits with top_k sampling</span>
        <span class="k">if</span> <span class="n">top_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Keep only top_k values</span>
            <span class="n">top_logits</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">top_k</span><span class="p">)</span>
            <span class="n">min_val</span> <span class="o">=</span> <span class="n">top_logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">logits</span> <span class="o">&lt;</span> <span class="n">min_val</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">logits</span><span class="p">)</span>

        <span class="c1"># New: Apply temperature scaling</span>
        <span class="k">if</span> <span class="n">temperature</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">:</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">/</span> <span class="n">temperature</span>

            <span class="c1"># Apply softmax to get probabilities</span>
            <span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch_size, context_len)</span>

            <span class="c1"># Sample from the distribution</span>
            <span class="n">idx_next</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch_size, 1)</span>

        <span class="c1"># Otherwise same as before: get idx of the vocab entry with the highest logits value</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">idx_next</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># (batch_size, 1)</span>

        <span class="c1"># Same as before: append sampled index to the running sequence</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx_next</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch_size, num_tokens+1)</span>

    <span class="k">return</span> <span class="n">idx</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>

<span class="n">token_ids</span> <span class="o">=</span> <span class="n">generate</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">idx</span><span class="o">=</span><span class="n">text_to_token_ids</span><span class="p">(</span><span class="s2">&quot;Every effort moves you&quot;</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">),</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
    <span class="n">context_size</span><span class="o">=</span><span class="n">GPT_CONFIG_124M</span><span class="p">[</span><span class="s2">&quot;ctx_len&quot;</span><span class="p">],</span>
    <span class="n">top_k</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">1.5</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output text:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">token_ids_to_text</span><span class="p">(</span><span class="n">token_ids</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Output text:
 Every effort moves you?&quot;,&quot; was down surprise. It is to face watching me by his painting him back to my work
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="loading-and-saving-model-weights-in-pytorch">
<h2><span class="section-number">6.4. </span>Loading and saving model weights in PyTorch<a class="headerlink" href="#loading-and-saving-model-weights-in-pytorch" title="Permalink to this headline">#</a></h2>
<p>Training LLMs is computationally expensive, so it’s crucial to be able to save and load LLM weights</p>
<p>The recommended way in PyTorch is to save the model weights, the so-called <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> via by applying the <code class="docutils literal notranslate"><span class="pre">torch.save</span></code> function to the  <code class="docutils literal notranslate"><span class="pre">.state_dict()</span></code> method:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s2">&quot;model.pth&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Then we can load the model weights into a new <code class="docutils literal notranslate"><span class="pre">GPTModel</span></code> model instance as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">GPTModel</span><span class="p">(</span><span class="n">GPT_CONFIG_124M</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;model.pth&quot;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">();</span>
</pre></div>
</div>
</div>
</div>
<p>It’s common to train LLMs with adaptive optimizers like Adam or AdamW instead of regular SGD
These adaptive optimizers store additional parameters for each model weight, so it makes sense to save them as well in case we plan to continue the pretraining later:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">({</span>
    <span class="s2">&quot;model_state_dict&quot;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
    <span class="s2">&quot;optimizer_state_dict&quot;</span><span class="p">:</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
    <span class="p">},</span> 
    <span class="s2">&quot;model_and_optimizer.pth&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;model_and_optimizer.pth&quot;</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">GPTModel</span><span class="p">(</span><span class="n">GPT_CONFIG_124M</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s2">&quot;model_state_dict&quot;</span><span class="p">])</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s2">&quot;optimizer_state_dict&quot;</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">();</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="loading-pretrained-weights-from-open-ai">
<h2><span class="section-number">6.5. </span>Loading pretrained weights from Open AI<a class="headerlink" href="#loading-pretrained-weights-from-open-ai" title="Permalink to this headline">#</a></h2>
<p>Fortunately, we don’t have to spend tens to hundreds of thousands of dollars to pretrain the model on a large pretraining corpus but can load the pretrained weights provided by OpenAI</p>
<p>First, some boilerplate code to download the files from OpenAI and load the weights into Python
Since OpenAI used <a class="reference external" href="https://www.tensorflow.org/">TensorFlow</a>, we will have to install and use TensorFlow for loading the weights; <a class="reference external" href="https://github.com/tqdm/tqdm">tqdm</a> is a progress bar library
Uncomment and run the next cell to install the required libraries</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># pip install tensorflow tqdm</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TensorFlow version:&quot;</span><span class="p">,</span> <span class="n">version</span><span class="p">(</span><span class="s2">&quot;tensorflow&quot;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;tqdm version:&quot;</span><span class="p">,</span> <span class="n">version</span><span class="p">(</span><span class="s2">&quot;tqdm&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>TensorFlow version: 2.15.0
tqdm version: 4.66.1
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Relative import from the gpt_download.py contained in this folder</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">gpt_download</span><span class="w"> </span><span class="kn">import</span> <span class="n">download_and_load_gpt2</span>
</pre></div>
</div>
</div>
</div>
<p>We can then download the model weights for the 124 million parameter model as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hparams</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">download_and_load_gpt2</span><span class="p">(</span><span class="n">model_size</span><span class="o">=</span><span class="s2">&quot;124M&quot;</span><span class="p">,</span> <span class="n">models_dir</span><span class="o">=</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>File already exists and is up-to-date: gpt2/124M/checkpoint
File already exists and is up-to-date: gpt2/124M/encoder.json
File already exists and is up-to-date: gpt2/124M/hparams.json
File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001
File already exists and is up-to-date: gpt2/124M/model.ckpt.index
File already exists and is up-to-date: gpt2/124M/model.ckpt.meta
File already exists and is up-to-date: gpt2/124M/vocab.bpe
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Settings:&quot;</span><span class="p">,</span> <span class="n">hparams</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Settings: {&#39;n_vocab&#39;: 50257, &#39;n_ctx&#39;: 1024, &#39;n_embd&#39;: 768, &#39;n_head&#39;: 12, &#39;n_layer&#39;: 12}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Parameter dictionary keys:&quot;</span><span class="p">,</span> <span class="n">params</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Parameter dictionary keys: dict_keys([&#39;blocks&#39;, &#39;b&#39;, &#39;g&#39;, &#39;wpe&#39;, &#39;wte&#39;])
</pre></div>
</div>
</div>
</div>
<p>Alternatively, “355M”, “774M”, and “1558M” are also supported <code class="docutils literal notranslate"><span class="pre">model_size</span></code> arguments
The difference between these differently sized models is summarized in the figure below:</p>
<img src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/llm/clipboard_20240409_035544.png" width=500px><p>Above, we loaded the 124M GPT-2 model weights into Python, however we still need to transfer them into our <code class="docutils literal notranslate"><span class="pre">GPTModel</span></code> instance
First, we initialize a new GPTModel instance
Note that the original GPT model initialized the linear layers for the query, key, and value matrices in the multi-head attention module with bias vectors, which is not required or recommended; however, to be able to load the weights correctly, we have to enable these too by setting <code class="docutils literal notranslate"><span class="pre">qkv_bias</span></code> to <code class="docutils literal notranslate"><span class="pre">True</span></code> in our implementation, too
We are also using the <code class="docutils literal notranslate"><span class="pre">1024</span></code> token context length that was used by the original GPT-2 model(s)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define model configurations in a dictionary for compactness</span>
<span class="n">model_configs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;gpt2-small (124M)&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">:</span> <span class="mi">768</span><span class="p">,</span> <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span> <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">12</span><span class="p">},</span>
    <span class="s2">&quot;gpt2-medium (355M)&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">:</span> <span class="mi">1024</span><span class="p">,</span> <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">24</span><span class="p">,</span> <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">16</span><span class="p">},</span>
    <span class="s2">&quot;gpt2-large (774M)&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">:</span> <span class="mi">1280</span><span class="p">,</span> <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">36</span><span class="p">,</span> <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">20</span><span class="p">},</span>
    <span class="s2">&quot;gpt2-xl (1558M)&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">:</span> <span class="mi">1600</span><span class="p">,</span> <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">48</span><span class="p">,</span> <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">25</span><span class="p">},</span>
<span class="p">}</span>

<span class="c1"># Copy the base configuration and update with specific model settings</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;gpt2-small (124M)&quot;</span>  <span class="c1"># Example model name</span>
<span class="n">NEW_CONFIG</span> <span class="o">=</span> <span class="n">GPT_CONFIG_124M</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">NEW_CONFIG</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">model_configs</span><span class="p">[</span><span class="n">model_name</span><span class="p">])</span>
<span class="n">NEW_CONFIG</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;ctx_len&quot;</span><span class="p">:</span> <span class="mi">1024</span><span class="p">,</span> <span class="s2">&quot;qkv_bias&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>

<span class="n">gpt</span> <span class="o">=</span> <span class="n">GPTModel</span><span class="p">(</span><span class="n">NEW_CONFIG</span><span class="p">)</span>
<span class="n">gpt</span><span class="o">.</span><span class="n">eval</span><span class="p">();</span>
</pre></div>
</div>
</div>
</div>
<p>The next task is to assign the OpenAI weights to the corresponding weight tensors in our <code class="docutils literal notranslate"><span class="pre">GPTModel</span></code> instance</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">assign</span><span class="p">(</span><span class="n">left</span><span class="p">,</span> <span class="n">right</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">left</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">right</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Shape mismatch. Left: </span><span class="si">{</span><span class="n">left</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, Right: </span><span class="si">{</span><span class="n">right</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">right</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">load_weights_into_gpt</span><span class="p">(</span><span class="n">gpt</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="n">gpt</span><span class="o">.</span><span class="n">pos_emb</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">assign</span><span class="p">(</span><span class="n">gpt</span><span class="o">.</span><span class="n">pos_emb</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;wpe&#39;</span><span class="p">])</span>
    <span class="n">gpt</span><span class="o">.</span><span class="n">tok_emb</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">assign</span><span class="p">(</span><span class="n">gpt</span><span class="o">.</span><span class="n">tok_emb</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;wte&#39;</span><span class="p">])</span>
    
    <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;blocks&quot;</span><span class="p">])):</span>
        <span class="n">q_w</span><span class="p">,</span> <span class="n">k_w</span><span class="p">,</span> <span class="n">v_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
            <span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;blocks&quot;</span><span class="p">][</span><span class="n">b</span><span class="p">][</span><span class="s2">&quot;attn&quot;</span><span class="p">][</span><span class="s2">&quot;c_attn&quot;</span><span class="p">])[</span><span class="s2">&quot;w&quot;</span><span class="p">],</span> <span class="mi">3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">gpt</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">b</span><span class="p">]</span><span class="o">.</span><span class="n">att</span><span class="o">.</span><span class="n">W_query</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">assign</span><span class="p">(</span>
            <span class="n">gpt</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">b</span><span class="p">]</span><span class="o">.</span><span class="n">att</span><span class="o">.</span><span class="n">W_query</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">q_w</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="n">gpt</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">b</span><span class="p">]</span><span class="o">.</span><span class="n">att</span><span class="o">.</span><span class="n">W_key</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">assign</span><span class="p">(</span>
            <span class="n">gpt</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">b</span><span class="p">]</span><span class="o">.</span><span class="n">att</span><span class="o">.</span><span class="n">W_key</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">k_w</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="n">gpt</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">b</span><span class="p">]</span><span class="o">.</span><span class="n">att</span><span class="o">.</span><span class="n">W_value</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">assign</span><span class="p">(</span>
            <span class="n">gpt</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">b</span><span class="p">]</span><span class="o">.</span><span class="n">att</span><span class="o">.</span><span class="n">W_value</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">v_w</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

        <span class="n">q_b</span><span class="p">,</span> <span class="n">k_b</span><span class="p">,</span> <span class="n">v_b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
            <span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;blocks&quot;</span><span class="p">][</span><span class="n">b</span><span class="p">][</span><span class="s2">&quot;attn&quot;</span><span class="p">][</span><span class="s2">&quot;c_attn&quot;</span><span class="p">])[</span><span class="s2">&quot;b&quot;</span><span class="p">],</span> <span class="mi">3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">gpt</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">b</span><span class="p">]</span><span class="o">.</span><span class="n">att</span><span class="o">.</span><span class="n">W_query</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">assign</span><span class="p">(</span>
            <span class="n">gpt</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">b</span><span class="p">]</span><span class="o">.</span><span class="n">att</span><span class="o">.</span><span class="n">W_query</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="n">q_b</span><span class="p">)</span>
        <span class="n">gpt</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">b</span><span class="p">]</span><span class="o">.</span><span class="n">att</span><span class="o">.</span><span class="n">W_key</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">assign</span><span class="p">(</span>
            <span class="n">gpt</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">b</span><span class="p">]</span><span class="o">.</span><span class="n">att</span><span class="o">.</span><span class="n">W_key</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="n">k_b</span><span class="p">)</span>
        <span class="n">gpt</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">b</span><span class="p">]</span><span class="o">.</span><span class="n">att</span><span class="o">.</span><span class="n">W_value</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">assign</span><span class="p">(</span>
            <span class="n">gpt</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">b</span><span class="p">]</span><span class="o">.</span><span class="n">att</span><span class="o">.</span><span class="n">W_value</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="n">v_b</span><span class="p">)</span>

        <span class="n">gpt</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">b</span><span class="p">]</span><span class="o">.</span><span class="n">att</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">assign</span><span class="p">(</span>
            <span class="n">gpt</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">b</span><span class="p">]</span><span class="o">.</span><span class="n">att</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> 
            <span class="n">params</span><span class="p">[</span><span class="s2">&quot;blocks&quot;</span><span class="p">][</span><span class="n">b</span><span class="p">][</span><span class="s2">&quot;attn&quot;</span><span class="p">][</span><span class="s2">&quot;c_proj&quot;</span><span class="p">][</span><span class="s2">&quot;w&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="n">gpt</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">b</span><span class="p">]</span><span class="o">.</span><span class="n">att</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">assign</span><span class="p">(</span>
            <span class="n">gpt</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">b</span><span class="p">]</span><span class="o">.</span><span class="n">att</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> 
            <span class="n">params</span><span class="p">[</span><span class="s2">&quot;blocks&quot;</span><span class="p">][</span><span class="n">b</span><span class="p">][</span><span class="s2">&quot;attn&quot;</span><span class="p">][</span><span class="s2">&quot;c_proj&quot;</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">])</span>

        <span class="n">gpt</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">b</span><span class="p">]</span><span class="o">.</span><span class="n">ff</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">assign</span><span class="p">(</span>
            <span class="n">gpt</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">b</span><span class="p">]</span><span class="o">.</span><span class="n">ff</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> 
            <span class="n">params</span><span class="p">[</span><span class="s2">&quot;blocks&quot;</span><span class="p">][</span><span class="n">b</span><span class="p">][</span><span class="s2">&quot;mlp&quot;</span><span class="p">][</span><span class="s2">&quot;c_fc&quot;</span><span class="p">][</span><span class="s2">&quot;w&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="n">gpt</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">b</span><span class="p">]</span><span class="o">.</span><span class="n">ff</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">assign</span><span class="p">(</span>
            <span class="n">gpt</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">b</span><span class="p">]</span><span class="o">.</span><span class="n">ff</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> 
            <span class="n">params</span><span class="p">[</span><span class="s2">&quot;blocks&quot;</span><span class="p">][</span><span class="n">b</span><span class="p">][</span><span class="s2">&quot;mlp&quot;</span><span class="p">][</span><span class="s2">&quot;c_fc&quot;</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">])</span>
        <span class="n">gpt</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">b</span><span class="p">]</span><span class="o">.</span><span class="n">ff</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">assign</span><span class="p">(</span>
            <span class="n">gpt</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">b</span><span class="p">]</span><span class="o">.</span><span class="n">ff</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> 
            <span class="n">params</span><span class="p">[</span><span class="s2">&quot;blocks&quot;</span><span class="p">][</span><span class="n">b</span><span class="p">][</span><span class="s2">&quot;mlp&quot;</span><span class="p">][</span><span class="s2">&quot;c_proj&quot;</span><span class="p">][</span><span class="s2">&quot;w&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="n">gpt</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">b</span><span class="p">]</span><span class="o">.</span><span class="n">ff</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">assign</span><span class="p">(</span>
            <span class="n">gpt</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">b</span><span class="p">]</span><span class="o">.</span><span class="n">ff</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> 
            <span class="n">params</span><span class="p">[</span><span class="s2">&quot;blocks&quot;</span><span class="p">][</span><span class="n">b</span><span class="p">][</span><span class="s2">&quot;mlp&quot;</span><span class="p">][</span><span class="s2">&quot;c_proj&quot;</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">])</span>

        <span class="n">gpt</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">b</span><span class="p">]</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">assign</span><span class="p">(</span>
            <span class="n">gpt</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">b</span><span class="p">]</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span> 
            <span class="n">params</span><span class="p">[</span><span class="s2">&quot;blocks&quot;</span><span class="p">][</span><span class="n">b</span><span class="p">][</span><span class="s2">&quot;ln_1&quot;</span><span class="p">][</span><span class="s2">&quot;g&quot;</span><span class="p">])</span>
        <span class="n">gpt</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">b</span><span class="p">]</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">shift</span> <span class="o">=</span> <span class="n">assign</span><span class="p">(</span>
            <span class="n">gpt</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">b</span><span class="p">]</span><span class="o">.</span><span class="n">norm1</span><span class="o">.</span><span class="n">shift</span><span class="p">,</span> 
            <span class="n">params</span><span class="p">[</span><span class="s2">&quot;blocks&quot;</span><span class="p">][</span><span class="n">b</span><span class="p">][</span><span class="s2">&quot;ln_1&quot;</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">])</span>
        <span class="n">gpt</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">b</span><span class="p">]</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">assign</span><span class="p">(</span>
            <span class="n">gpt</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">b</span><span class="p">]</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span> 
            <span class="n">params</span><span class="p">[</span><span class="s2">&quot;blocks&quot;</span><span class="p">][</span><span class="n">b</span><span class="p">][</span><span class="s2">&quot;ln_2&quot;</span><span class="p">][</span><span class="s2">&quot;g&quot;</span><span class="p">])</span>
        <span class="n">gpt</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">b</span><span class="p">]</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">shift</span> <span class="o">=</span> <span class="n">assign</span><span class="p">(</span>
            <span class="n">gpt</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="n">b</span><span class="p">]</span><span class="o">.</span><span class="n">norm2</span><span class="o">.</span><span class="n">shift</span><span class="p">,</span> 
            <span class="n">params</span><span class="p">[</span><span class="s2">&quot;blocks&quot;</span><span class="p">][</span><span class="n">b</span><span class="p">][</span><span class="s2">&quot;ln_2&quot;</span><span class="p">][</span><span class="s2">&quot;b&quot;</span><span class="p">])</span>

    <span class="n">gpt</span><span class="o">.</span><span class="n">final_norm</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">assign</span><span class="p">(</span><span class="n">gpt</span><span class="o">.</span><span class="n">final_norm</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span> <span class="n">params</span><span class="p">[</span><span class="s2">&quot;g&quot;</span><span class="p">])</span>
    <span class="n">gpt</span><span class="o">.</span><span class="n">final_norm</span><span class="o">.</span><span class="n">shift</span> <span class="o">=</span> <span class="n">assign</span><span class="p">(</span><span class="n">gpt</span><span class="o">.</span><span class="n">final_norm</span><span class="o">.</span><span class="n">shift</span><span class="p">,</span> <span class="n">params</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">])</span>
    <span class="n">gpt</span><span class="o">.</span><span class="n">out_head</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">assign</span><span class="p">(</span><span class="n">gpt</span><span class="o">.</span><span class="n">out_head</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">params</span><span class="p">[</span><span class="s2">&quot;wte&quot;</span><span class="p">])</span>
    
    
<span class="n">load_weights_into_gpt</span><span class="p">(</span><span class="n">gpt</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
<span class="n">gpt</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>If the model is loaded correctly, we can use it to generate new text using our previous <code class="docutils literal notranslate"><span class="pre">generate</span></code> function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>

<span class="n">token_ids</span> <span class="o">=</span> <span class="n">generate</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">gpt</span><span class="p">,</span>
    <span class="n">idx</span><span class="o">=</span><span class="n">text_to_token_ids</span><span class="p">(</span><span class="s2">&quot;Every effort moves you&quot;</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">),</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
    <span class="n">context_size</span><span class="o">=</span><span class="n">NEW_CONFIG</span><span class="p">[</span><span class="s2">&quot;ctx_len&quot;</span><span class="p">],</span>
    <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">1.5</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output text:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">token_ids_to_text</span><span class="p">(</span><span class="n">token_ids</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Output text:
 Every effort moves you toward finding an ideal new way to practice something!

What makes us want to be on top of that?
</pre></div>
</div>
</div>
</div>
</section>
<section id="acknowledgments">
<h2><span class="section-number">6.6. </span>Acknowledgments<a class="headerlink" href="#acknowledgments" title="Permalink to this headline">#</a></h2>
<p>Thanks to <a class="reference external" href="https://github.com/rasbt">Sebastian Raschka</a> for creating the open-source course <a class="reference external" href="https://github.com/rasbt/LLMs-from-scratch">Build a Large Language Model (From Scratch)</a>. It inspires the majority of the content in this chapter.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "penjc/llmbook",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./pretrained-model"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>
    <script type="text/javascript">
        function init() {
            WaveDrom.ProcessAll();
        }
        window.onload = init;
    </script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="implementing-a-GPT-model.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">5. </span>Implementing a GPT model</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../finetuning/finetuning-for-text-classification.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">7. </span>Finetuning for Text Classification</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By 彭健程<br/>
  
      &copy; Copyright 2024.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>